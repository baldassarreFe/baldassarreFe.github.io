<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Federico Baldassarre</title><link>https://baldassarrefe.github.io/</link><atom:link href="https://baldassarrefe.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Federico Baldassarre</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 14 Aug 2025 00:00:00 +0000</lastBuildDate><image><url>https://baldassarrefe.github.io/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_512x512_fill_lanczos_center_3.png</url><title>Federico Baldassarre</title><link>https://baldassarrefe.github.io/</link></image><item><title>DINOv3</title><link>https://baldassarrefe.github.io/publication/dinov3-2025/</link><pubDate>Thu, 14 Aug 2025 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/dinov3-2025/</guid><description>&lt;h1 id="dinov3-">DINOv3 ðŸ¦–ðŸ¦–ðŸ¦–&lt;/h1>
&lt;p>DINOv3 is a significant leap forward in self-supervised vision foundation models.
This major release sets a new standard, offering stunning &lt;strong>high-resolution dense features&lt;/strong> that are set to revolutionize various vision tasks.&lt;/p>
&lt;p>While we scaled both model size and training data, DINOv3&amp;rsquo;s true innovation lies in key architectural and training advancements.&lt;/p>
&lt;h2 id="whats-inside-dinov3">What&amp;rsquo;s inside DINOv3?&lt;/h2>
&lt;p>DINOv3 is packed with features and techniques:&lt;/p>
&lt;ul>
&lt;li>A powerful &lt;strong>7B ViT&lt;/strong> foundation model, complemented by smaller distilled models.&lt;/li>
&lt;li>Trained on a massive dataset of 1.7 billion curated images, remarkably &lt;strong>without any annotations&lt;/strong>, showcasing the power of self-supervision.&lt;/li>
&lt;li>The introduction of &lt;strong>Gram anchoring&lt;/strong>, a novel technique that effectively addresses feature map degradation, a common challenge when training models of this scale for extended periods.&lt;/li>
&lt;li>Achieving &lt;strong>high-resolution adaptation&lt;/strong> through the use of relative spatial coordinates and 2D RoPE, enabling exceptional performance on high-resolution inputs.&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./scaling-perf.png" alt="Scaling performance of the DINOv3" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h3 id="dense-feature-degradation-and-gram-anchoring">Dense Feature Degradation and Gram Anchoring&lt;/h3>
&lt;p>As we scaled up model size, number of images, and training duration, we observed two phenomena:&lt;/p>
&lt;ul>
&lt;li>Downstream classification accuracy continued to improve.&lt;/li>
&lt;li>Performance on dense tasks, which rely on rich, consistent feature maps, would rapidly drop.&lt;/li>
&lt;/ul>
&lt;p>For example, see the comparison of ImageNet-1K classification accuracy &lt;em>vs.&lt;/em> Pascal VOC segmentation mIoU:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./segdrop.png" alt="Segdrop" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>From qualitative observations, we notice that early during training, dense features are nicely localized and consistent within object boundaris.
However, as training progresses, they become progressively more noisy.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./feature-degradation.png" alt="Feature map degradation" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>To overcome this roadblock, we introduce &lt;strong>Gram anchoring&lt;/strong>.
Our goal is to achieve consistency found in early training iterations while retaining the semantic richness developed during late training.
Therefore, we load an early checkpoint of the model being trained, and use is as a second teacher.
Instead of forcing the features of the student to match the teacher features perfectly, which would be a too strong constraint, we distill the high-resolution &lt;strong>pairwise patch similarity&lt;/strong>, i.e. the Gram matrix, from the teacher.
Critically, this still allows the student features to evolve freely and improve, enabling unprecedented performance.&lt;/p>
&lt;p>Upon introducing Gram anchoring, we qualitatively observe an improvement in the feature maps:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./ducks-before-after.png" alt="Ducks before and after Gram anchoring" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>And quantitatively, we see a significant boost in performance on dense tasks:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./gram-before-after.png" alt="Performance before and after Gram anchoring" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h3 id="high-resolution-adaptation">High-Resolution Adaptation&lt;/h3>
&lt;p>Now that we can train the model for longer, we should also think about training it on high-resolution images.
For the longest part, we feed the model images of size $256{\times}256$, which we increase to $512{\times}512$ and $768{\times}768$ in the final training stages.&lt;/p>
&lt;p>Our &lt;strong>constant-schedule training&lt;/strong> approach allows us to spin off a model specifically for high-resolution adaptation at any point.
We seamlessly increase the input resolution while keeping Gram anchoring enabled.
In our implementation, we use &lt;strong>relative spatial coordinates&lt;/strong> in the range of $[0,1]^2$ to represent the position of each patch in the image for the 2D rotational positional encoding (RoPE).
As a consequence, images of higher resolution are simply treated as an interpolation of the patch coordinates, without special handling.&lt;/p>
&lt;h2 id="how-well-does-it-work">How well does it work?&lt;/h2>
&lt;h3 id="qualitative-visualization">Qualitative Visualization&lt;/h3>
&lt;p>Compared to other vision foundation models, both self-supervised and not, DINOv3 features are more consistent and semantically rich.
The quality of DINOv3&amp;rsquo;s feature maps is truly exceptional.&lt;/p>
&lt;p>In the crowded market scene below, we visualize the cosine similarity between the patches marked with a red cross and all other patches.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./market.jpg" alt="Crowded market scene" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Compared to other vision foundation models, both self-supervised and not, DINOv3 patch features are more consistent, artifact-free, with no unwanted spilling around the edges of objects.
Object contours look so precise, it appears like object segmentation is just one clustering step away!&lt;/p>
&lt;p>Below, we visualize the first three PCA components of the features as RGB:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./pca-dinosaur-bike.jpg" alt="Feature PCA dinosaur and bike" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>After high-resolution adaptation, DINOv3 models generalize well to a wide range of resolutions, even higher than those seen during training.
Forget Full-HD, it&amp;rsquo;s time for &lt;strong>dense features in 4K&lt;/strong>!&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./high-res-dog.jpg" alt="DINOv3 dense features in 4K" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h3 id="performance-evaluation">Performance Evaluation&lt;/h3>
&lt;p>Even with a frozen backbone, DINOv3 delivers massive performance gains across a variety of benchmarks.
Some examples include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>COCO detection&lt;/strong>: achieving a state-of-the-art 66.1 mAP with a frozen backbone and detection model based on Plain-DETR. This is incredible!&lt;/li>
&lt;li>&lt;strong>ADE20k segmentation&lt;/strong>: a linear 55.9 mIoU, a significant +6 improvement over previous self-supervised methods, and 63.0 mIoU with a decoder on top.&lt;/li>
&lt;li>&lt;strong>3D correspondence&lt;/strong>: impressive 64.4 recall on NAVI.&lt;/li>
&lt;li>&lt;strong>Video tracking&lt;/strong>: a strong 83.3 J&amp;amp;F on DAVIS, thanks to its ability to handle high-resolution inputs.&lt;/li>
&lt;/ul>
&lt;p>Below, is an example of segmentation tracking on a video.
Object instances in the first frame of the video are segmented manually (thanks SAM 2!).
Subsequent frames are automatically segmented by propagating labels from the first frame based on patch similarity in the DINOv3 feature space.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./ducks-tracking.jpg" alt="Segmentation tracking example" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="model-family">Model Family&lt;/h2>
&lt;p>DINOv3 comes with a versatile model family to suit various needs:&lt;/p>
&lt;ul>
&lt;li>The powerful &lt;strong>ViT-7B base model&lt;/strong>.&lt;/li>
&lt;li>A range of ViT variants: ViT-S/S+/B/L/H+ with parameters ranging from 21M to 840M.&lt;/li>
&lt;li>&lt;strong>ConvNeXt variants&lt;/strong> optimized for efficient inference.&lt;/li>
&lt;li>A &lt;strong>text-aligned ViT-L (dino.txt)&lt;/strong>, opening up exciting possibilities for multi-modal applications.&lt;/li>
&lt;/ul>
&lt;p>Also, by popular demand, we switched to &lt;strong>patch size 16&lt;/strong>!
No more patch size 14 and weird image sizes like $224{\times}224$ and $448{\times}448$.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./family-flops.png" alt="DINOv3 model family" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The DINOv3 recipe exhibits remarkable generalization beyond natural images.
We train a model from scratch on aerial images, setting a state of the art on geospatial benchmarks like canopy height estimation and land cover classification.
This demonstrates the pure magic of self-supervised learning: the same recipe, applied to a different domain, yields incredible results.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./satellite-tasks.png" alt="Geospatial tasks" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>We are committed to open-source and we release the DINOv3 models with a free commercial license.
Find everything you need to get started:&lt;/p>
&lt;ul>
&lt;li>Training and evaluation code, adapters, and notebooks in the &lt;a href="https://github.com/facebookresearch/dinov3" target="_blank" rel="noopener">GitHub repository&lt;/a>.&lt;/li>
&lt;li>Pretrained backbones in this &lt;a href="https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009" target="_blank" rel="noopener">HF collection&lt;/a> compatible with the Transformers library.&lt;/li>
&lt;li>The official &lt;a href="https://ai.meta.com/blog/dinov3-self-supervised-vision-model/" target="_blank" rel="noopener">blog post&lt;/a>.&lt;/li>
&lt;li>The &lt;a href="https://arxiv.org/abs/2508.10104" target="_blank" rel="noopener">research paper&lt;/a> on arXiv.&lt;/li>
&lt;/ul>
&lt;h2 id="twitter-thread">Twitter thread&lt;/h2>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Say hello to DINOv3 ðŸ¦–ðŸ¦–ðŸ¦–&lt;br>&lt;br>A major release that raises the bar of self-supervised vision foundation models.&lt;br>With stunning high-resolution dense features, itâ€™s a game-changer for vision tasks!&lt;br>&lt;br>We scaled model size and training data, but here&amp;#39;s what makes it special ðŸ‘‡ &lt;a href="https://t.co/VBkRuAIOCi">pic.twitter.com/VBkRuAIOCi&lt;/a>&lt;/p>&amp;mdash; Federico Baldassarre (@BaldassarreFe) &lt;a href="https://twitter.com/BaldassarreFe/status/1956027867860516867?ref_src=twsrc%5Etfw">August 14, 2025&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Back to the Features: DINO as a Foundation for Video World Models</title><link>https://baldassarrefe.github.io/publication/dino-world-2025/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/dino-world-2025/</guid><description>&lt;p>Given past frames, the model can predict future DINOv2 features autoregressively.
How well does it do? It depends on the objects in motion and the context provided.&lt;/p>
&lt;p>In the video below, the model can observe the taxi trajectory for long enough to predict that it will turn right.&lt;/p>
&lt;figure id="figure-autoregressive-prediction-of-dino-world-with-10-frames-of-context-top-row-ground-truth-video-middle-row-per-frame-dinov2-features-bottom-row-autoregressive-predictions-of-dino-world">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Autoregressive prediction of DINO-world with 10 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world." srcset="
/publication/dino-world-2025/autoregressive_taxi_10_hue4871d0d37b3e474f89ea0efdc740fbe_2238195_427fefce7f2c57f1a98b4845df2780f7.webp 400w,
/publication/dino-world-2025/autoregressive_taxi_10_hue4871d0d37b3e474f89ea0efdc740fbe_2238195_94ce584be6b1d409b3080522cb194a44.webp 760w,
/publication/dino-world-2025/autoregressive_taxi_10_hue4871d0d37b3e474f89ea0efdc740fbe_2238195_1200x1200_fit_q80_h2_lanczos_3.webp 1200w"
src="https://baldassarrefe.github.io/publication/dino-world-2025/autoregressive_taxi_10_hue4871d0d37b3e474f89ea0efdc740fbe_2238195_427fefce7f2c57f1a98b4845df2780f7.webp"
width="760"
height="148"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Autoregressive prediction of DINO-world with 10 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world.
&lt;/figcaption>&lt;/figure>
&lt;p>On the contrary, when given a too few frames, the model predicts that the taxi will continue straight.
The blurry features towards the end are an artifact of the model&amp;rsquo;s uncertainty about the future due to the ambiguous context.&lt;/p>
&lt;figure id="figure-autoregressive-prediction-of-dino-world-with-only-4-frames-of-context-top-row-ground-truth-video-middle-row-per-frame-dinov2-features-bottom-row-autoregressive-predictions-of-dino-world">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Autoregressive prediction of DINO-world with only 4 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world." srcset="
/publication/dino-world-2025/autoregressive_taxi_4_hu70925a527a10997dd20e96e54ca285dc_2298178_fd0dcab6282978c42d276745cdf39c9b.webp 400w,
/publication/dino-world-2025/autoregressive_taxi_4_hu70925a527a10997dd20e96e54ca285dc_2298178_d4d370e25d1a68dfe956ad7ba98555e2.webp 760w,
/publication/dino-world-2025/autoregressive_taxi_4_hu70925a527a10997dd20e96e54ca285dc_2298178_1200x1200_fit_q80_h2_lanczos_3.webp 1200w"
src="https://baldassarrefe.github.io/publication/dino-world-2025/autoregressive_taxi_4_hu70925a527a10997dd20e96e54ca285dc_2298178_fd0dcab6282978c42d276745cdf39c9b.webp"
width="760"
height="149"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Autoregressive prediction of DINO-world with only 4 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world.
&lt;/figcaption>&lt;/figure>
&lt;hr>
&lt;p>&lt;span style="font-size: .8rem">An early version of this work, without the action-conditioned fine-tuning and the planning tasks,
was presented at the ICML 2025 Workshop on
&lt;a href="https://physical-world-modeling.github.io/" target="_blank" rel="noopener">Building Physically Plausible World Models&lt;/a>.&lt;/span>&lt;/p></description></item><item><title>DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment</title><link>https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/</link><pubDate>Wed, 11 Jun 2025 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/</guid><description>&lt;!-- Example of zero-shot segmentation using DINO.txt: -->
&lt;!--
&lt;figure id="figure-zero-shot-segmentation-using-dinotxt">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Zero-shot segmentation using DINO.txt" srcset="
/publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_b118d9c55be2d8f0ea4e22e4c448a407.webp 400w,
/publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_e46ca33f9f7ec699094abe9dfbad06e5.webp 760w,
/publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_1200x1200_fit_q80_h2_lanczos_3.webp 1200w"
src="https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_b118d9c55be2d8f0ea4e22e4c448a407.webp"
width="760"
height="254"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Zero-shot segmentation using DINO.txt
&lt;/figcaption>&lt;/figure>
-->
&lt;h2 id="twitter-thread">Twitter thread&lt;/h2>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">DINOv2 meets text at &lt;a href="https://twitter.com/hashtag/CVPR?src=hash&amp;amp;ref_src=twsrc%5Etfw">#CVPR&lt;/a> 2025! Why choose between high-quality DINO features and CLIP-style vision-language alignment? Pick both with dino.txt ðŸ¦–ðŸ“–&lt;br>&lt;br>We align frozen DINOv2 features with text captions, obtaining both image-level and patch-level alignment at a minimal cost. [1/N] &lt;a href="https://t.co/7BTwLxqXNG">pic.twitter.com/7BTwLxqXNG&lt;/a>&lt;/p>&amp;mdash; Federico Baldassarre (@BaldassarreFe) &lt;a href="https://twitter.com/BaldassarreFe/status/1933975376377892974?ref_src=twsrc%5Etfw">June 14, 2025&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;h2 id="video-presentation">Video presentation&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/qNsAgsvhbw4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Cluster and Predict Latents Patches for Improved Masked Image Modeling</title><link>https://baldassarrefe.github.io/publication/capi-tmlr-2025/</link><pubDate>Sun, 01 Jun 2025 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/capi-tmlr-2025/</guid><description>&lt;p>Here is a tweet-sized summary by the lead author:&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Want strong SSL, but not the complexity of DINOv2?&lt;br>&lt;br>CAPI: Cluster and Predict Latents Patches for Improved Masked Image Modeling. &lt;a href="https://t.co/gOB4QO7DKn">pic.twitter.com/gOB4QO7DKn&lt;/a>&lt;/p>&amp;mdash; TimDarcet (@TimDarcet) &lt;a href="https://twitter.com/TimDarcet/status/1890389871543419255?ref_src=twsrc%5Etfw">February 14, 2025&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Variable Rate Allocation for Vector-Quantized Autoencoders</title><link>https://baldassarrefe.github.io/publication/semantic-compression-icassp-2023/</link><pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/semantic-compression-icassp-2023/</guid><description>&lt;h2 id="video-presentation">Video presentation&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/JggN9r1FFMw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Learnable Masked Tokens for Improved Transferability of Self-Supervised Vision Transformers</title><link>https://baldassarrefe.github.io/publication/learnable-masked-token-ecmlpkdd-2022/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/learnable-masked-token-ecmlpkdd-2022/</guid><description/></item><item><title>Quantitative Metrics for Evaluating Explanations of Video DeepFake Detectors</title><link>https://baldassarrefe.github.io/publication/deepfake-explanations-bmvc-2022/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/deepfake-explanations-bmvc-2022/</guid><description>&lt;h2 id="video-presentation">Video presentation&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/KAwigBozHeM" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Towards Self-Supervised Learning of Global and Object-Centric Representations</title><link>https://baldassarrefe.github.io/publication/object-centric-ssl-iclrw-2022/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/object-centric-ssl-iclrw-2022/</guid><description/></item><item><title>Modeling SARS-CoV-2 proteins in the CASP-commons experiment</title><link>https://baldassarrefe.github.io/publication/casp-covid19-proteins-2021/</link><pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/casp-covid19-proteins-2021/</guid><description/></item><item><title>Explanation-based Weakly-supervised Learning of Visual Relations with Graph Networks</title><link>https://baldassarrefe.github.io/publication/weakly-sup-relationship-detection-eccv-2020/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/weakly-sup-relationship-detection-eccv-2020/</guid><description>&lt;h2 id="video-presentation">Video presentation&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/7n7ffyRTuog" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>GraphQA: Protein Model Quality Assessment Using Graph Convolutional Networks</title><link>https://baldassarrefe.github.io/publication/graphqa-bioinformatics-2020/</link><pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/graphqa-bioinformatics-2020/</guid><description/></item><item><title>Explainability Techniques for Graph Convolutional Networks</title><link>https://baldassarrefe.github.io/publication/explainability-gcn-icmlw-2019/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/explainability-gcn-icmlw-2019/</guid><description/></item><item><title>Morphing Architectures for Pose-based Image Generation of People in Clothing</title><link>https://baldassarrefe.github.io/publication/zalando-master-thesis-2018/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/zalando-master-thesis-2018/</guid><description/></item><item><title>Deep Koalarization</title><link>https://baldassarrefe.github.io/publication/deep-koalarization-2017/</link><pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/deep-koalarization-2017/</guid><description/></item><item><title>Experience</title><link>https://baldassarrefe.github.io/experience/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/experience/</guid><description/></item><item><title>News</title><link>https://baldassarrefe.github.io/news/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/news/</guid><description>&lt;!-- REMEMBER TO SYNC THE TOP NEWS WITH content/_index.md -->
&lt;h2 id="2025">2025&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>October 2025&lt;/strong>
Guest lecture on DINOv3 to deep learning students at Chalmers University, Gothenburg, Sweden ðŸ‡¸ðŸ‡ª&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>September 2025&lt;/strong>
Honored to be invited to present DINOv3 at the &lt;a href="https://partphys-indico.unige.ch/event/1944/overview" target="_blank" rel="noopener">Foundation Model workshop&lt;/a> at the Geneva Institute of Theoretical Science ðŸ‡¨ðŸ‡­&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>September 2025&lt;/strong>
Presenting my postdoc contributions (CAPI, dino.txt, DINO-world and DINOv3) to students of the WASP AI program at the University of LinkÃ¶ping, Sweden ðŸ‡¸ðŸ‡ª&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>August 2025&lt;/strong>
Announcing &lt;a href="https://baldassarrefe.github.io/publication/dinov3-2025/">DINOv3&lt;/a>, a major release that raises the bar of self-supervised vision foundation models ðŸ¦–ðŸ¦–ðŸ¦–&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>July 2025&lt;/strong>
Publishing &lt;a href="https://baldassarrefe.github.io/publication/dino-world-2025/">DINO-world&lt;/a>, a latent video world model
trained in the feature space of DINOv2. Also presented at &lt;a href="https://icml.cc/Conferences/2025" target="_blank" rel="noopener">ICML 2025&lt;/a>
in Vancouver ðŸ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>June 2025&lt;/strong>
See you in Nashville, Tennessee, for &lt;a href="https://cvpr2025.thecvf.com/" target="_blank" rel="noopener">CVPR 2025&lt;/a> ðŸ¤ ðŸª•&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>May 2025&lt;/strong>
Our work on scalable pre-training of dense image representations, &lt;a href="https://baldassarrefe.github.io/publication/capi-tmlr-2025/">CAPI&lt;/a>,
is accepted at &lt;a href="https://jmlr.org/tmlr/" target="_blank" rel="noopener">TMLR&lt;/a>.
Congrats to &lt;a href="https://x.com/timdarcet" target="_blank" rel="noopener">TimothÃ©e&lt;/a> for leading this work!&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Feb 2025&lt;/strong>
Our &lt;a href="https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/">&amp;ldquo;DINO.txt&amp;rdquo;&lt;/a> paper on aligning vision and language
representations is accepted at &lt;a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank" rel="noopener">CVPR 2025&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="2024">2024&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Oct 2024&lt;/strong>
Presenting my work on obtaining object-centric representations by clustering DINOv2 features at the FAIR conference in New York.&lt;/li>
&lt;/ul>
&lt;h2 id="2023">2023&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Sept 2023&lt;/strong>
I&amp;rsquo;m moving to Paris to join the DINO team at &lt;a href="https://ai.meta.com/" target="_blank" rel="noopener">Meta AI&lt;/a> as a postdoctoral researcher ðŸ¦–&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Jun 2023&lt;/strong>
PhD Graduation at KTH, Stockholm ðŸŽ“ðŸŽ‰&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Feb 2023&lt;/strong>
Our work on semantic image compression with quantized autoencoders is accepted at &lt;a href="https://2023.ieeeicassp.org/" target="_blank" rel="noopener">ICASSP 2023&lt;/a> (Meta AI internship).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="2022">2022&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Sept 2022&lt;/strong>
Our work on quantitative metrics for evaluating DeepFake explanations is accepted at &lt;a href="https://bmvc2022.org/" target="_blank" rel="noopener">BMVC 2022&lt;/a> (Huawei Research internship).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Aug 2022&lt;/strong>
My master student &lt;a href="https://www.linkedin.com/in/erikdao/" target="_blank" rel="noopener">Erik Dao&lt;/a> graduated with a thesis on sparsity in transformers for object detection (DETR).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Jun 2022&lt;/strong>
I&amp;rsquo;m starting a research internship at &lt;a href="https://ai.meta.com/" target="_blank" rel="noopener">FAIR (Meta AI)&lt;/a> in Paris under the supervision of &lt;a href="https://scholar.google.com/citations?user=1lcY2z4AAAAJ" target="_blank" rel="noopener">HervÃ© JÃ©gou&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Mar 2022&lt;/strong>
Our work on object-centric learning is accepted at the &lt;a href="https://objects-structure-causality.github.io/" target="_blank" rel="noopener">ICLR 2022 Workshop on the Elements of Reasoning: Objects, Structure, and Causality&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Jan 2022&lt;/strong>
Gave a &lt;a href="https://docs.google.com/presentation/d/e/2PACX-1vSQZflpI3hoB2pjSLcdofUEqnnNH87JNMc4PbQ5JwxmOSiEvUbGJU22ZmGivyBFFYYO79Mytqfv09CO/pub?start=false&amp;amp;loop=false" target="_blank" rel="noopener">presentation&lt;/a> on evaluating DeepFake explanations at Digital Future&amp;rsquo;s &lt;a href="https://www.digitalfutures.kth.se/event/machine-learning-day/" target="_blank" rel="noopener">Machine Learning Day&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="2021">2021&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Sept 2021&lt;/strong>
Started a summer internship at &lt;a href="https://www.linkedin.com/company/huawei-ireland-research-center/" target="_blank" rel="noopener">Huawei Research&lt;/a> in Dublin!&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Jan 2021&lt;/strong>
My master student &lt;a href="https://www.linkedin.com/in/jindong-wu-2240231b1/" target="_blank" rel="noopener">Jindong Wu&lt;/a> graduated with a thesis on pooling strategies for Graph Networks.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="2020-and-older">2020 and older&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Aug 2020&lt;/strong>
Proud to be nominated as an outstanding reviewer at &lt;a href="https://eccv2020.eu/" target="_blank" rel="noopener">ECCV 2020&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Jun 2020&lt;/strong>
I hosted a full-day &lt;a href="https://github.com/baldassarreFe/rpl-workshop" target="_blank" rel="noopener">workshop&lt;/a> for my department on how get the most from our in-house GPU cluster.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>May 2019&lt;/strong>
Our work on GNN Explainability is accepted at the &lt;a href="https://graphreason.github.io/" target="_blank" rel="noopener">ICML 2019 Workshop on Learning and Reasoning with Graph-Structured Data&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Jan 2019&lt;/strong>
Presented my Master&amp;rsquo;s thesis on image generation for clothes virtual try-on at the &lt;a href="https://www.nldl.org/history/previous-nldl-iterations#h.4dksogkwrfhx" target="_blank" rel="noopener">Northern Lights Deep Learning Workshop&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Sept 2018&lt;/strong>
Starting my PhD journey at KTH under the supervision of &lt;a href="https://www.kth.se/profile/azizpour" target="_blank" rel="noopener">Prof. Hossein Azizpour&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul></description></item></channel></rss>