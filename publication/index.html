<!doctype html><!-- This site was created with Hugo Blox. https://hugoblox.com --><!-- Last Published: October 15, 2025 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script><link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.bc2bd53044e06eaf49628846a4ccee6f.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script async src="https://www.googletagmanager.com/gtag/js?id=UA-168997432-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","UA-168997432-1",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Federico Baldassarre"><meta name=description content="Personal website of Federico Baldassarre"><link rel=alternate hreflang=en-us href=https://baldassarrefe.github.io/publication/><link rel=canonical href=https://baldassarrefe.github.io/publication/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#707070"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@BaldassarreFe"><meta property="twitter:creator" content="@BaldassarreFe"><meta property="twitter:image" content="https://baldassarrefe.github.io/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_512x512_fill_lanczos_center_3.png"><meta property="og:type" content="website"><meta property="og:site_name" content="Federico Baldassarre"><meta property="og:url" content="https://baldassarrefe.github.io/publication/"><meta property="og:title" content="Publications | Federico Baldassarre"><meta property="og:description" content="Personal website of Federico Baldassarre"><meta property="og:image" content="https://baldassarrefe.github.io/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2025-08-14T00:00:00+00:00"><link rel=alternate href=/publication/index.xml type=application/rss+xml title="Federico Baldassarre"><title>Publications | Federico Baldassarre</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=3a079e7dad19be978a318345a7749d34><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Federico Baldassarre</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Federico Baldassarre</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/experience><span>Experience</span></a></li><li class=nav-item><a class="nav-link active" href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=https://drive.google.com/file/d/1ngJjnnBx__RNjO9pz7etgx72e98iYHLJ/view target=_blank rel=noopener><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Publications</h1></div><div class=universal-wrapper><div class=row><div class=col-lg-12><div class=article-style><p>A brief summary of my publications, also available on <a href="https://scholar.google.com/citations?user=0iy5EucAAAAJ" target=_blank rel=noopener>Google Scholar</a>.<br> </p></div><div class="form-row mb-4"><div class=col-auto><input type=search class="filter-search form-control form-control-sm" placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off role=textbox spellcheck=false></div><div class=col-auto><select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group=pubtype><option value=*>Type</option><option value=.pubtype-article-journal>Journal article</option><option value=.pubtype-paper-conference>Conference paper</option><option value=.pubtype-preprint>Preprint</option><option value=.pubtype-report>Report</option><option value=.pubtype-thesis>Thesis</option></select></div><div class=col-auto><select class="pub-filters form-control form-control-sm" data-filter-group=year><option value=*>Date</option><option value=.year-2025>2025</option><option value=.year-2023>2023</option><option value=.year-2022>2022</option><option value=.year-2021>2021</option><option value=.year-2020>2020</option><option value=.year-2019>2019</option><option value=.year-2018>2018</option><option value=.year-2017>2017</option></select></div></div><div id=container-publications><div class="grid-sizer col-lg-12 isotope-item pubtype-preprint year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/dinov3-2025/>DINOv3</a></div><a href=/publication/dinov3-2025/ class=summary-link><div class=article-style><strong>Preprint 2025</strong> - Scaling vision SSL to 7B parameters and 1.7B images, achieving unprecedented patch feature quality.</div></a><div class="stream-meta article-metadata"><div><span>Oriane Siméoni</span>, <span>Huy v. Vo</span>, <span>Maximilian Seitzer</span>, <span class=author-highlighted>Federico Baldassarre</span>, <span>Maxime Oquab</span>, <span>Cijo Jose</span>, <span>Vasil Khalidov</span>, <span>Marc Szafraniec</span>, <span>Seungeun Yi</span>, <span>Michaël Ramamonjisoa</span>, <span>Francisco Massa</span>, <span>Daniel Haziza</span>, <span>Luca Wehrstedt</span>, <span>Jianyuan Wang</span>, <span>Timothée Darcet</span>, <span>Théo Moutakanni</span>, <span>Leonel Sentana</span>, <span>Claire Roberts</span>, <span>Andrea Vedaldi</span>, <span>Jamie Tolan</span>, <span>John Brandt</span>, <span>Camille Couprie</span>, <span>Julien Mairal</span>, <span>Hervé Jégou</span>, <span>Patrick Labatut</span>, <span>Piotr Bojanowski</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2508.10104 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/dinov3-2025/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/facebookresearch/dinov3 target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ai.meta.com/dinov3/ target=_blank rel=noopener>Project</a></div></div><div class=ml-3><a href=/publication/dinov3-2025/><img src=/publication/dinov3-2025/featured_hua2e51d8738e76bcd2adce16b61ceb6db_1746329_8b03ec95ee51a90860aa8868f3f6c18d.webp height=150 width=150 alt=DINOv3 loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-paper-conference year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/dino-world-2025/>Back to the Features: DINO as a Foundation for Video World Models</a></div><a href=/publication/dino-world-2025/ class=summary-link><div class=article-style><strong>ICML Workshop 2025</strong> - Learning physical world models in the latent space of DINOv2 from uncurated web videos.</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Federico Baldassarre</span>, <span>Marc Szafraniec</span>, <span>Basile Terver</span>, <span>Vasil Khalidov</span>, <span>Francisco Massa</span>, <span>Yann LeCun</span>, <span>Patrick Labatut</span>, <span>Maximilian Seitzer</span>, <span>Piotr Bojanowski</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2507.19468 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/dino-world-2025/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/dino-world-2025/><img src=/publication/dino-world-2025/featured_hu2a99b56dfdc7f896e7f4ac055d4f8ae6_1113671_1bac3dbaae886967c1a7fcb655bdc109.webp height=72 width=150 alt="Back to the Features: DINO as a Foundation for Video World Models" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-paper-conference year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/dino-txt-cvpr-2025/>DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment</a></div><a href=/publication/dino-txt-cvpr-2025/ class=summary-link><div class=article-style><strong>CVPR 2025</strong> - Locked-image tuning for vision-language alignment using a DINOv2 backbone and a few tricks on top.</div></a><div class="stream-meta article-metadata"><div><span>Cijo Jose</span>, <span>Théo Moutakanni</span>, <span>Dahyun Kang</span>, <span class=author-highlighted>Federico Baldassarre</span>, <span>Timothée Darcet</span>, <span>Hu Xu</span>, <span>Daniel Li</span>, <span>Marc Szafraniec</span>, <span>Michaël Ramamonjisoa</span>, <span>Maxime Oquab</span>, <span>Oriane Siméoni</span>, <span>Huy v. Vo</span>, <span>Patrick Labatut</span>, <span>Piotr Bojanowski</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2412.16334 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/dino-txt-cvpr-2025/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/facebookresearch/dinov2 target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/qNsAgsvhbw4 target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=/publication/dino-txt-cvpr-2025/><img src=/publication/dino-txt-cvpr-2025/featured_hu87b83078ac845ab8da1311cb537ea0a5_421037_51d73d4238fa87c79fdaf2106a48c7ce.webp height=150 width=150 alt="DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-article-journal year-2025"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/capi-tmlr-2025/>Cluster and Predict Latents Patches for Improved Masked Image Modeling</a></div><a href=/publication/capi-tmlr-2025/ class=summary-link><div class=article-style><strong>TMLR 2025</strong> - Stable training of dense image representations using a clustering loss on ViT patch tokens.</div></a><div class="stream-meta article-metadata"><div><span>Timothée Darcet</span>, <span class=author-highlighted>Federico Baldassarre</span>, <span>Maxime Oquab</span>, <span>Julien Mairal</span>, <span>Piotr Bojanowski</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2502.08769 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/capi-tmlr-2025/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/facebookresearch/capi target=_blank rel=noopener>Code</a></div></div><div class=ml-3><a href=/publication/capi-tmlr-2025/><img src=/publication/capi-tmlr-2025/featured_hubfcb49287c849db914c65a15404ac575_1185210_1a0c10756cc8aef8f140d54b24a9e6a1.webp height=109 width=150 alt="Cluster and Predict Latents Patches for Improved Masked Image Modeling" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-paper-conference year-2023"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/semantic-compression-icassp-2023/>Variable Rate Allocation for Vector-Quantized Autoencoders</a></div><a href=/publication/semantic-compression-icassp-2023/ class=summary-link><div class=article-style><strong>ICASSP 2023</strong> - Control the bit rate allocation of a a PQ-VAE using semantic information from DINO, in collaboration with Meta AI in Paris.</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Federico Baldassarre</span>, <span>Alaaeldin El-Nouby</span>, <span>Hervé Jégou</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/document/10095451 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/semantic-compression-icassp-2023/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/e/2PACX-1vS2Fp92AW4tbD7xxykbZh0dVQng8kRxIh0Ijs32Cn6azeZRs9YVKJhz8Ya0iApIL6-pyOBHcsrf5lcQ/pub?start=false&amp;loop=true" target=_blank rel=noopener>Poster
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://youtu.be/JggN9r1FFMw target=_blank rel=noopener>Video</a></div></div><div class=ml-3><a href=/publication/semantic-compression-icassp-2023/><img src=/publication/semantic-compression-icassp-2023/featured_hu8212aa1b231d7169e3889b9e3014e6ec_881157_23fe202702cbb23b6236ce1acc72b3f4.webp height=93 width=150 alt="Variable Rate Allocation for Vector-Quantized Autoencoders" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-paper-conference year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/deepfake-explanations-bmvc-2022/>Quantitative Metrics for Evaluating Explanations of Video DeepFake Detectors</a></div><a href=/publication/deepfake-explanations-bmvc-2022/ class=summary-link><div class=article-style><strong>BMVC 2022</strong> - Quantitative metrics for evaluating explanations of video DeepFake detectors, in collaboration with the Huawei Ireland Research Center.</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Federico Baldassarre</span>, <span>Quentin Debard</span>, <span>Gonzalo Fiz Pontiveros</span>, <span>Tri Kurniawan Wijaya</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2210.03683 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/deepfake-explanations-bmvc-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/baldassarreFe/deepfake-detection target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/e/2PACX-1vSVlXecqnVuj2swsyMmPYD-Lxv7OtdESagPOHdjHAr2YAmfYSQ1GmW3t3tBQKoLetKanBv-kv6Ortwx/pub?start=false&amp;loop=true" target=_blank rel=noopener>Poster
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=KAwigBozHeM" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.48550/arXiv.2210.03683 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/deepfake-explanations-bmvc-2022/><img src=/publication/deepfake-explanations-bmvc-2022/featured_hu8cdc290cbd032c2a0c341ee0bf3135f3_109552_d196ad5e475111b6fab1ad27af1b66fb.webp height=76 width=150 alt="Quantitative Metrics for Evaluating Explanations of Video DeepFake Detectors" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-paper-conference year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/learnable-masked-token-ecmlpkdd-2022/>Learnable Masked Tokens for Improved Transferability of Self-Supervised Vision Transformers</a></div><a href=/publication/learnable-masked-token-ecmlpkdd-2022/ class=summary-link><div class=article-style><strong>ECML PKDD 2022</strong> - Learnable masked tokens reduce overfitting when training transformers in low-data regime.</div></a><div class="stream-meta article-metadata"><div><span>Hao Hu</span>, <span class=author-highlighted>Federico Baldassarre</span>, <span>Hossein Azizpour</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://2022.ecmlpkdd.org/wp-content/uploads/2022/09/sub_792.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/learnable-masked-token-ecmlpkdd-2022/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/learnable-masked-token-ecmlpkdd-2022/><img src=/publication/learnable-masked-token-ecmlpkdd-2022/featured_hud9ba893eb462c847c419f1ffef3435b1_778575_276c2a27fac2d2cb236d9035fb64851c.webp height=53 width=150 alt="Learnable Masked Tokens for Improved Transferability of Self-Supervised Vision Transformers" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-paper-conference year-2022"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/object-centric-ssl-iclrw-2022/>Towards Self-Supervised Learning of Global and Object-Centric Representations</a></div><a href=/publication/object-centric-ssl-iclrw-2022/ class=summary-link><div class=article-style><strong>ICLR Workshop 2022</strong> - How to learn object-centric representations using contrastive learning on multi-object datasets?</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Federico Baldassarre</span>, <span>Hossein Azizpour</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.05997 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/object-centric-ssl-iclrw-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/baldassarreFe/iclr-osc-22 target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/e/2PACX-1vRW0msEbXehQvD8eKrngzzdDw6aQyH4FWLh3fJ7fWzuDd8fazFzSLTbRdjUn6F_Qr069sq1C-H6XBx4/pub?start=false&amp;loop=true" target=_blank rel=noopener>Poster
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.48550/arXiv.2203.05997 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/object-centric-ssl-iclrw-2022/><img src=/publication/object-centric-ssl-iclrw-2022/featured_hudd37b9f33fdd4a041230d71a42762819_179835_7bc60da2ad69c867143de0b03bf24a93.webp height=55 width=150 alt="Towards Self-Supervised Learning of Global and Object-Centric Representations" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-article-journal year-2021"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/casp-covid19-proteins-2021/>Modeling SARS-CoV-2 proteins in the CASP-commons experiment</a></div><a href=/publication/casp-covid19-proteins-2021/ class=summary-link><div class=article-style><strong>Proteins: Structure, Function, and Bioinformatics</strong> - CASP community-wide experiment on modeling SARS-CoV-2 proteins causing the COVID-19 pandemic.</div></a><div class="stream-meta article-metadata"><div><span>Andriy Kryshtafovych</span>, <span>John Moult</span>, <span>Wendy Billings</span>, <span>Dennis Della Corte</span>, <span>Krzysztof Fidelis</span>, <span>Sohee Kwon</span>, <span>Kliment Olechnovič</span>, <span>Chaok Seok</span>, <span>Česlovas Venclovas</span>, <span>Jonghun Won</span>, <span>CASP-COVID Participants</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Individual contributors to the protein modeling effort"></i></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.26231 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/casp-covid19-proteins-2021/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1002/prot.26231 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/casp-covid19-proteins-2021/><img src=/publication/casp-covid19-proteins-2021/featured_hua1368a7c4f146d66fee0d1b0ddc86d00_875429_9166db6f36ceb09b66de0822729ba2de.webp height=150 width=150 alt="Modeling SARS-CoV-2 proteins in the CASP-commons experiment" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-paper-conference year-2020"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/weakly-sup-relationship-detection-eccv-2020/>Explanation-based Weakly-supervised Learning of Visual Relations with Graph Networks</a></div><a href=/publication/weakly-sup-relationship-detection-eccv-2020/ class=summary-link><div class=article-style><strong>ECCV 2020</strong> - A novel weakly-supervised method for visual relationship detection that relies on minimal image-level predicate labels, graph networks, and attribution-based explanations.</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Federico Baldassarre</span>, <span>Kevin Smith</span>, <span>Josephine Sullivan</span>, <span>Hossein Azizpour</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2006.09562 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/weakly-sup-relationship-detection-eccv-2020/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/baldassarreFe/ws-vrd target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=7n7ffyRTuog" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-58604-1_37 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/weakly-sup-relationship-detection-eccv-2020/><img src=/publication/weakly-sup-relationship-detection-eccv-2020/featured_hu54437f0c0e61f703c0870a71518a742a_232036_103edeafeb8b1ac1f1ba48133300517c.webp height=50 width=150 alt="Explanation-based Weakly-supervised Learning of Visual Relations with Graph Networks" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-article-journal year-2020"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/graphqa-bioinformatics-2020/>GraphQA: Protein Model Quality Assessment Using Graph Convolutional Networks</a></div><a href=/publication/graphqa-bioinformatics-2020/ class=summary-link><div class=article-style><strong>Bioinformatics 2020</strong> - A novel method for protein quality assessment based on a graphical representation of tertiary structures and graph convolutional networks.</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Federico Baldassarre</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>David Menéndez Hurtado</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Arne Elofsson</span>, <span>Hossein Azizpour</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btaa714/34192500/btaa714.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/graphqa-bioinformatics-2020/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/baldassarreFe/graphqa target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1093/bioinformatics/btaa714 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/graphqa-bioinformatics-2020/><img src=/publication/graphqa-bioinformatics-2020/featured_hu2b332b5b50b13342dbe0dbbeb638c9f0_333661_b18b5ffdc1d3b0a67af31b406279252d.webp height=156 width=150 alt="GraphQA: Protein Model Quality Assessment Using Graph Convolutional Networks" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-paper-conference year-2019"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/explainability-gcn-icmlw-2019/>Explainability Techniques for Graph Convolutional Networks</a></div><a href=/publication/explainability-gcn-icmlw-2019/ class=summary-link><div class=article-style><strong>ICML Workshop 2019</strong> - How do Sensitivity Analysis, Guided Backpropagation, and Layer-Wise Relevance Propagation behave when applied to Graph Neural Networks?</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Federico Baldassarre</span>, <span>Hossein Azizpour</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1905.13686 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/explainability-gcn-icmlw-2019/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/baldassarreFe/graph-network-explainability target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://docs.google.com/presentation/d/e/2PACX-1vQwSxAtA7jbiLOdNg9RTHe8FItX8WHYlSaR-_qciKzVwS8rvvsNEMCwjtmgBhIELC9FEX45rvORIi9I/pub?start=false&amp;loop=true" target=_blank rel=noopener>Poster
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/https://doi.org/10.48550/arXiv.1905.13686 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/explainability-gcn-icmlw-2019/><img src=/publication/explainability-gcn-icmlw-2019/featured_huff83fa14c08b17a828c0db4ec1d3f7ae_124617_378330a62096086446db233a7e35d4f2.webp height=92 width=150 alt="Explainability Techniques for Graph Convolutional Networks" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-thesis year-2018"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/zalando-master-thesis-2018/>Morphing Architectures for Pose-based Image Generation of People in Clothing</a></div><a href=/publication/zalando-master-thesis-2018/ class=summary-link><div class=article-style><strong>Master&rsquo;s Thesis</strong> - Given the picture of a person and of a clothing item, generate the image of the person wearing it. Work performed during an internship at Zalando Research, Berlin.</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Federico Baldassarre</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://kth.diva-portal.org/smash/get/diva2:1239446/FULLTEXT01.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zalando-master-thesis-2018/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/zalando-master-thesis-2018/><img src=/publication/zalando-master-thesis-2018/featured_hu203815271b064b6655e206632cf91dd1_351746_4bd31f671a5f906816df2ef6ccc74d93.webp height=133 width=150 alt="Morphing Architectures for Pose-based Image Generation of People in Clothing" loading=lazy></a></div></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-report year-2017"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/deep-koalarization-2017/>Deep Koalarization</a></div><a href=/publication/deep-koalarization-2017/ class=summary-link><div class=article-style><strong>Technical Report</strong> - Grayscale image colorization using a CNN conditione on Inception-ResNet-v2. Deep Learning course at KTH, best project award.</div></a><div class="stream-meta article-metadata"><div><span class=author-highlighted>Federico Baldassarre</span>, <span>Diego Gonzalez Morín</span>, <span>Lucas Rodés-Guirao</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1712.03400 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/deep-koalarization-2017/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/baldassarreFe/deep-koalarization target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.48550/arXiv.1712.03400 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/deep-koalarization-2017/><img src=/publication/deep-koalarization-2017/featured_huc53f872b0d1bcce3404f1a98f7c43f3b_207532_374388a0c6f8e2743245dafcbf1e726b.webp height=59 width=150 alt="Deep Koalarization" loading=lazy></a></div></div></div></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 Federico Baldassarre | License <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by></p></footer></div></div><script src=/js/vendor-bundle.min.b2240102cb8b24bcbe037562ce2ea60a.js></script><script src=https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.7f5ebaff62ae468cff8bb3dd1337bb9b.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js type=module></script></body></html>