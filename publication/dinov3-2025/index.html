<!doctype html><!-- This site was created with Hugo Blox. https://hugoblox.com --><!-- Last Published: October 15, 2025 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script><link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.bc2bd53044e06eaf49628846a4ccee6f.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script async src="https://www.googletagmanager.com/gtag/js?id=UA-168997432-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","UA-168997432-1",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Federico Baldassarre"><meta name=description content="**Preprint 2025** - Scaling vision SSL to 7B parameters and 1.7B images, achieving unprecedented patch feature quality.
"><link rel=alternate hreflang=en-us href=https://baldassarrefe.github.io/publication/dinov3-2025/><link rel=canonical href=https://baldassarrefe.github.io/publication/dinov3-2025/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#707070"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@BaldassarreFe"><meta property="twitter:creator" content="@BaldassarreFe"><meta property="twitter:image" content="https://baldassarrefe.github.io/publication/dinov3-2025/featured.jpg"><meta property="og:type" content="article"><meta property="og:site_name" content="Federico Baldassarre"><meta property="og:url" content="https://baldassarrefe.github.io/publication/dinov3-2025/"><meta property="og:title" content="DINOv3 | Federico Baldassarre"><meta property="og:description" content="**Preprint 2025** - Scaling vision SSL to 7B parameters and 1.7B images, achieving unprecedented patch feature quality.
"><meta property="og:image" content="https://baldassarrefe.github.io/publication/dinov3-2025/featured.jpg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2025-08-14T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-14T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://baldassarrefe.github.io/publication/dinov3-2025/"},"headline":"DINOv3","image":["https://baldassarrefe.github.io/publication/dinov3-2025/featured.jpg"],"datePublished":"2025-08-14T00:00:00Z","dateModified":"2025-08-14T00:00:00Z","author":{"@type":"Person","name":"Oriane SimÃ©oni"},"publisher":{"@type":"Organization","name":"Federico Baldassarre","logo":{"@type":"ImageObject","url":"https://baldassarrefe.github.io/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_192x192_fill_lanczos_center_3.png"}},"description":"**Preprint 2025** - Scaling vision SSL to 7B parameters and 1.7B images, achieving unprecedented patch feature quality.\n"}</script><title>DINOv3 | Federico Baldassarre</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=36ad703bd43fcf091b2184c4cef0f773><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Federico Baldassarre</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Federico Baldassarre</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/experience><span>Experience</span></a></li><li class=nav-item><a class="nav-link active" href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=https://drive.google.com/file/d/1ngJjnnBx__RNjO9pz7etgx72e98iYHLJ/view target=_blank rel=noopener><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>DINOv3</h1><div class=article-metadata><div><span>Oriane SimÃ©oni</span>, <span>Huy v. Vo</span>, <span>Maximilian Seitzer</span>, <span class=author-highlighted>Federico Baldassarre</span>, <span>Maxime Oquab</span>, <span>Cijo Jose</span>, <span>Vasil Khalidov</span>, <span>Marc Szafraniec</span>, <span>Seungeun Yi</span>, <span>MichaÃ«l Ramamonjisoa</span>, <span>Francisco Massa</span>, <span>Daniel Haziza</span>, <span>Luca Wehrstedt</span>, <span>Jianyuan Wang</span>, <span>TimothÃ©e Darcet</span>, <span>ThÃ©o Moutakanni</span>, <span>Leonel Sentana</span>, <span>Claire Roberts</span>, <span>Andrea Vedaldi</span>, <span>Jamie Tolan</span>, <span>John Brandt</span>, <span>Camille Couprie</span>, <span>Julien Mairal</span>, <span>HervÃ© JÃ©gou</span>, <span>Patrick Labatut</span>, <span>Piotr Bojanowski</span></div><span class=article-date>August, 2025</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2508.10104 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/dinov3-2025/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header" href=https://github.com/facebookresearch/dinov3 target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header" href=https://ai.meta.com/dinov3/ target=_blank rel=noopener>Project</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:720px><div style=position:relative><img src=/publication/dinov3-2025/featured_hua2e51d8738e76bcd2adce16b61ceb6db_1746329_d80cf566df5d9bd9f56976bd6dbd58d9.webp width=720 height=720 alt class=featured-image>
<span class=article-header-caption>DINOv3 dense features on natural and aerial images.</span></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures.
By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images &ndash; using a single algorithm.
This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies.
First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization.
Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules.
Finally, we apply post-hoc strategies that further enhance our models&rsquo; flexibility with respect to resolution, model size, and alignment with text.
As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning.
DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models.
We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#preprint>Preprint</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">Preprint</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><h1 id=dinov3->DINOv3 ðŸ¦–ðŸ¦–ðŸ¦–</h1><p>DINOv3 is a significant leap forward in self-supervised vision foundation models.
This major release sets a new standard, offering stunning <strong>high-resolution dense features</strong> that are set to revolutionize various vision tasks.</p><p>While we scaled both model size and training data, DINOv3&rsquo;s true innovation lies in key architectural and training advancements.</p><h2 id=whats-inside-dinov3>What&rsquo;s inside DINOv3?</h2><p>DINOv3 is packed with features and techniques:</p><ul><li>A powerful <strong>7B ViT</strong> foundation model, complemented by smaller distilled models.</li><li>Trained on a massive dataset of 1.7 billion curated images, remarkably <strong>without any annotations</strong>, showcasing the power of self-supervision.</li><li>The introduction of <strong>Gram anchoring</strong>, a novel technique that effectively addresses feature map degradation, a common challenge when training models of this scale for extended periods.</li><li>Achieving <strong>high-resolution adaptation</strong> through the use of relative spatial coordinates and 2D RoPE, enabling exceptional performance on high-resolution inputs.</li></ul><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./scaling-perf.png alt="Scaling performance of the DINOv3" loading=lazy data-zoomable></div></div></figure></p><h3 id=dense-feature-degradation-and-gram-anchoring>Dense Feature Degradation and Gram Anchoring</h3><p>As we scaled up model size, number of images, and training duration, we observed two phenomena:</p><ul><li>Downstream classification accuracy continued to improve.</li><li>Performance on dense tasks, which rely on rich, consistent feature maps, would rapidly drop.</li></ul><p>For example, see the comparison of ImageNet-1K classification accuracy <em>vs.</em> Pascal VOC segmentation mIoU:</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./segdrop.png alt=Segdrop loading=lazy data-zoomable></div></div></figure></p><p>From qualitative observations, we notice that early during training, dense features are nicely localized and consistent within object boundaris.
However, as training progresses, they become progressively more noisy.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./feature-degradation.png alt="Feature map degradation" loading=lazy data-zoomable></div></div></figure></p><p>To overcome this roadblock, we introduce <strong>Gram anchoring</strong>.
Our goal is to achieve consistency found in early training iterations while retaining the semantic richness developed during late training.
Therefore, we load an early checkpoint of the model being trained, and use is as a second teacher.
Instead of forcing the features of the student to match the teacher features perfectly, which would be a too strong constraint, we distill the high-resolution <strong>pairwise patch similarity</strong>, i.e. the Gram matrix, from the teacher.
Critically, this still allows the student features to evolve freely and improve, enabling unprecedented performance.</p><p>Upon introducing Gram anchoring, we qualitatively observe an improvement in the feature maps:</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./ducks-before-after.png alt="Ducks before and after Gram anchoring" loading=lazy data-zoomable></div></div></figure></p><p>And quantitatively, we see a significant boost in performance on dense tasks:</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./gram-before-after.png alt="Performance before and after Gram anchoring" loading=lazy data-zoomable></div></div></figure></p><h3 id=high-resolution-adaptation>High-Resolution Adaptation</h3><p>Now that we can train the model for longer, we should also think about training it on high-resolution images.
For the longest part, we feed the model images of size $256{\times}256$, which we increase to $512{\times}512$ and $768{\times}768$ in the final training stages.</p><p>Our <strong>constant-schedule training</strong> approach allows us to spin off a model specifically for high-resolution adaptation at any point.
We seamlessly increase the input resolution while keeping Gram anchoring enabled.
In our implementation, we use <strong>relative spatial coordinates</strong> in the range of $[0,1]^2$ to represent the position of each patch in the image for the 2D rotational positional encoding (RoPE).
As a consequence, images of higher resolution are simply treated as an interpolation of the patch coordinates, without special handling.</p><h2 id=how-well-does-it-work>How well does it work?</h2><h3 id=qualitative-visualization>Qualitative Visualization</h3><p>Compared to other vision foundation models, both self-supervised and not, DINOv3 features are more consistent and semantically rich.
The quality of DINOv3&rsquo;s feature maps is truly exceptional.</p><p>In the crowded market scene below, we visualize the cosine similarity between the patches marked with a red cross and all other patches.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./market.jpg alt="Crowded market scene" loading=lazy data-zoomable></div></div></figure></p><p>Compared to other vision foundation models, both self-supervised and not, DINOv3 patch features are more consistent, artifact-free, with no unwanted spilling around the edges of objects.
Object contours look so precise, it appears like object segmentation is just one clustering step away!</p><p>Below, we visualize the first three PCA components of the features as RGB:</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./pca-dinosaur-bike.jpg alt="Feature PCA dinosaur and bike" loading=lazy data-zoomable></div></div></figure></p><p>After high-resolution adaptation, DINOv3 models generalize well to a wide range of resolutions, even higher than those seen during training.
Forget Full-HD, it&rsquo;s time for <strong>dense features in 4K</strong>!</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./high-res-dog.jpg alt="DINOv3 dense features in 4K" loading=lazy data-zoomable></div></div></figure></p><h3 id=performance-evaluation>Performance Evaluation</h3><p>Even with a frozen backbone, DINOv3 delivers massive performance gains across a variety of benchmarks.
Some examples include:</p><ul><li><strong>COCO detection</strong>: achieving a state-of-the-art 66.1 mAP with a frozen backbone and detection model based on Plain-DETR. This is incredible!</li><li><strong>ADE20k segmentation</strong>: a linear 55.9 mIoU, a significant +6 improvement over previous self-supervised methods, and 63.0 mIoU with a decoder on top.</li><li><strong>3D correspondence</strong>: impressive 64.4 recall on NAVI.</li><li><strong>Video tracking</strong>: a strong 83.3 J&amp;F on DAVIS, thanks to its ability to handle high-resolution inputs.</li></ul><p>Below, is an example of segmentation tracking on a video.
Object instances in the first frame of the video are segmented manually (thanks SAM 2!).
Subsequent frames are automatically segmented by propagating labels from the first frame based on patch similarity in the DINOv3 feature space.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./ducks-tracking.jpg alt="Segmentation tracking example" loading=lazy data-zoomable></div></div></figure></p><h2 id=model-family>Model Family</h2><p>DINOv3 comes with a versatile model family to suit various needs:</p><ul><li>The powerful <strong>ViT-7B base model</strong>.</li><li>A range of ViT variants: ViT-S/S+/B/L/H+ with parameters ranging from 21M to 840M.</li><li><strong>ConvNeXt variants</strong> optimized for efficient inference.</li><li>A <strong>text-aligned ViT-L (dino.txt)</strong>, opening up exciting possibilities for multi-modal applications.</li></ul><p>Also, by popular demand, we switched to <strong>patch size 16</strong>!
No more patch size 14 and weird image sizes like $224{\times}224$ and $448{\times}448$.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./family-flops.png alt="DINOv3 model family" loading=lazy data-zoomable></div></div></figure></p><p>The DINOv3 recipe exhibits remarkable generalization beyond natural images.
We train a model from scratch on aerial images, setting a state of the art on geospatial benchmarks like canopy height estimation and land cover classification.
This demonstrates the pure magic of self-supervised learning: the same recipe, applied to a different domain, yields incredible results.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./satellite-tasks.png alt="Geospatial tasks" loading=lazy data-zoomable></div></div></figure></p><p>We are committed to open-source and we release the DINOv3 models with a free commercial license.
Find everything you need to get started:</p><ul><li>Training and evaluation code, adapters, and notebooks in the <a href=https://github.com/facebookresearch/dinov3 target=_blank rel=noopener>GitHub repository</a>.</li><li>Pretrained backbones in this <a href=https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009 target=_blank rel=noopener>HF collection</a> compatible with the Transformers library.</li><li>The official <a href=https://ai.meta.com/blog/dinov3-self-supervised-vision-model/ target=_blank rel=noopener>blog post</a>.</li><li>The <a href=https://arxiv.org/abs/2508.10104 target=_blank rel=noopener>research paper</a> on arXiv.</li></ul><h2 id=twitter-thread>Twitter thread</h2><blockquote class=twitter-tweet><p lang=en dir=ltr>Say hello to DINOv3 ðŸ¦–ðŸ¦–ðŸ¦–<br><br>A major release that raises the bar of self-supervised vision foundation models.<br>With stunning high-resolution dense features, itâ€™s a game-changer for vision tasks!<br><br>We scaled model size and training data, but here's what makes it special ðŸ‘‡ <a href=https://t.co/VBkRuAIOCi>pic.twitter.com/VBkRuAIOCi</a></p>&mdash; Federico Baldassarre (@BaldassarreFe) <a href="https://twitter.com/BaldassarreFe/status/1956027867860516867?ref_src=twsrc%5Etfw">August 14, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script></div><div class=article-tags><a class="badge badge-light" href=/tag/self-supervised-learning/>Self-Supervised Learning</a>
<a class="badge badge-light" href=/tag/postdoc/>Postdoc</a>
<a class="badge badge-light" href=/tag/dino/>DINO</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdinov3-2025%2F&amp;text=DINOv3" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdinov3-2025%2F&amp;t=DINOv3" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=DINOv3&amp;body=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdinov3-2025%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdinov3-2025%2F&amp;title=DINOv3" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=DINOv3%20https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdinov3-2025%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdinov3-2025%2F&amp;title=DINOv3" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://baldassarrefe.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu248acd790418c168d502f2067d78835d_686148_270x270_fill_lanczos_center_3.png alt="Federico Baldassarre"></a><div class=media-body><h5 class=card-title><a href=https://baldassarrefe.github.io/>Federico Baldassarre</a></h5><h6 class=card-subtitle>Postdoctoral Researcher</h6><p class=card-text>Self-supervised learning, computer vision, and world models.</p><ul class=network-icon aria-hidden=true><li><a href=https://mailhide.io/e/nmVtbPdX target=_blank rel=noopener><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/baldassarreFe target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://twitter.com/BaldassarreFe target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://www.linkedin.com/in/federicobaldassarre target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href="https://scholar.google.com/citations?user=0iy5EucAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/publication/dino-world-2025/>Back to the Features: DINO as a Foundation for Video World Models</a></li><li><a href=/publication/capi-tmlr-2025/>Cluster and Predict Latents Patches for Improved Masked Image Modeling</a></li><li><a href=/publication/dino-txt-cvpr-2025/>DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment</a></li><li><a href=/publication/object-centric-ssl-iclrw-2022/>Towards Self-Supervised Learning of Global and Object-Centric Representations</a></li><li><a href=/publication/semantic-compression-icassp-2023/>Variable Rate Allocation for Vector-Quantized Autoencoders</a></li></ul></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">Â© 2025 Federico Baldassarre | License <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by></p></footer></div></div><script src=/js/vendor-bundle.min.b2240102cb8b24bcbe037562ce2ea60a.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.7f5ebaff62ae468cff8bb3dd1337bb9b.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js type=module></script></body></html>