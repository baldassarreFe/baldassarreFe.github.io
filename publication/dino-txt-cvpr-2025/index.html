<!doctype html><!-- This site was created with Hugo Blox. https://hugoblox.com --><!-- Last Published: October 15, 2025 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script><link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.bc2bd53044e06eaf49628846a4ccee6f.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script async src="https://www.googletagmanager.com/gtag/js?id=UA-168997432-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","UA-168997432-1",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Federico Baldassarre"><meta name=description content="**CVPR 2025** - Locked-image tuning for vision-language alignment using a DINOv2 backbone and a few tricks on top.
"><link rel=alternate hreflang=en-us href=https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/><link rel=canonical href=https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#707070"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@BaldassarreFe"><meta property="twitter:creator" content="@BaldassarreFe"><meta property="twitter:image" content="https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/featured.png"><meta property="og:type" content="article"><meta property="og:site_name" content="Federico Baldassarre"><meta property="og:url" content="https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/"><meta property="og:title" content="DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment | Federico Baldassarre"><meta property="og:description" content="**CVPR 2025** - Locked-image tuning for vision-language alignment using a DINOv2 backbone and a few tricks on top.
"><meta property="og:image" content="https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2025-05-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-11T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/"},"headline":"DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment","image":["https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/featured.png"],"datePublished":"2025-05-25T00:00:00Z","dateModified":"2025-06-11T00:00:00Z","author":{"@type":"Person","name":"Cijo Jose"},"publisher":{"@type":"Organization","name":"Federico Baldassarre","logo":{"@type":"ImageObject","url":"https://baldassarrefe.github.io/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_192x192_fill_lanczos_center_3.png"}},"description":"**CVPR 2025** - Locked-image tuning for vision-language alignment using a DINOv2 backbone and a few tricks on top.\n"}</script><title>DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment | Federico Baldassarre</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=496ddea7a9b6018400c314b417bb5bfb><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Federico Baldassarre</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Federico Baldassarre</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/experience><span>Experience</span></a></li><li class=nav-item><a class="nav-link active" href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=https://drive.google.com/file/d/1ngJjnnBx__RNjO9pz7etgx72e98iYHLJ/view target=_blank rel=noopener><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment</h1><div class=article-metadata><div><span>Cijo Jose</span>, <span>ThÃ©o Moutakanni</span>, <span>Dahyun Kang</span>, <span class=author-highlighted>Federico Baldassarre</span>, <span>TimothÃ©e Darcet</span>, <span>Hu Xu</span>, <span>Daniel Li</span>, <span>Marc Szafraniec</span>, <span>MichaÃ«l Ramamonjisoa</span>, <span>Maxime Oquab</span>, <span>Oriane SimÃ©oni</span>, <span>Huy v. Vo</span>, <span>Patrick Labatut</span>, <span>Piotr Bojanowski</span></div><span class=article-date>June, 2025</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2412.16334 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/dino-txt-cvpr-2025/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header" href=https://github.com/facebookresearch/dinov2 target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header" href=https://youtu.be/qNsAgsvhbw4 target=_blank rel=noopener>Video</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:720px><div style=position:relative><img src=/publication/dino-txt-cvpr-2025/featured_hu87b83078ac845ab8da1311cb537ea0a5_421037_2953c3df194f185724e13e3f8c16bdab.webp width=720 height=720 alt class=featured-image>
<span class=article-header-caption>Locked-image tuning using a frozen DINOv2 backbone.</span></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Self-supervised visual foundation models produce powerful embeddings that achieve remarkable performance on a wide range of downstream tasks.
However, unlike vision-language models such as CLIP, self-supervised visual features are not readily aligned with language, hindering their adoption in open-vocabulary tasks.
Our method, named <code>DINO.txt</code>, unlocks this new ability for DINOv2, a widely used self-supervised visual encoder.
We build upon the LiT training strategy, which trains a text encoder to align with a frozen vision model but leads to unsatisfactory results on dense tasks.
We propose several key ingredients to improve performance on both global and dense tasks, such as concatenating the <code>[CLS]</code> token with the patch average to train the alignment and curating data using both text and image modalities.
With these, we successfully train a CLIP-like model with only a fraction of the computational cost compared to CLIP while achieving state-of-the-art results in zero-shot classification and open-vocabulary semantic segmentation.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#paper-conference>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">Computer Vision and Pattern Recognition</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><!-- Example of zero-shot segmentation using DINO.txt: --><!-- 

















<figure  id="figure-zero-shot-segmentation-using-dinotxt">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="Zero-shot segmentation using DINO.txt" srcset="
               /publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_b118d9c55be2d8f0ea4e22e4c448a407.webp 400w,
               /publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_e46ca33f9f7ec699094abe9dfbad06e5.webp 760w,
               /publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_1200x1200_fit_q80_h2_lanczos_3.webp 1200w"
               src="/publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_b118d9c55be2d8f0ea4e22e4c448a407.webp"
               width="760"
               height="254"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      Zero-shot segmentation using DINO.txt
    </figcaption></figure>
 --><h2 id=twitter-thread>Twitter thread</h2><blockquote class=twitter-tweet><p lang=en dir=ltr>DINOv2 meets text at <a href="https://twitter.com/hashtag/CVPR?src=hash&amp;ref_src=twsrc%5Etfw">#CVPR</a> 2025! Why choose between high-quality DINO features and CLIP-style vision-language alignment? Pick both with dino.txt ðŸ¦–ðŸ“–<br><br>We align frozen DINOv2 features with text captions, obtaining both image-level and patch-level alignment at a minimal cost. [1/N] <a href=https://t.co/7BTwLxqXNG>pic.twitter.com/7BTwLxqXNG</a></p>&mdash; Federico Baldassarre (@BaldassarreFe) <a href="https://twitter.com/BaldassarreFe/status/1933975376377892974?ref_src=twsrc%5Etfw">June 14, 2025</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><h2 id=video-presentation>Video presentation</h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/qNsAgsvhbw4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></div><div class=article-tags><a class="badge badge-light" href=/tag/vision-language-alignment/>Vision-Language Alignment</a>
<a class="badge badge-light" href=/tag/llms/>LLMs</a>
<a class="badge badge-light" href=/tag/clip/>CLIP</a>
<a class="badge badge-light" href=/tag/postdoc/>Postdoc</a>
<a class="badge badge-light" href=/tag/dino/>DINO</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdino-txt-cvpr-2025%2F&amp;text=DINOv2+Meets+Text%3A+A+Unified+Framework+for+Image-+and+Pixel-Level+Vision-Language+Alignment" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdino-txt-cvpr-2025%2F&amp;t=DINOv2+Meets+Text%3A+A+Unified+Framework+for+Image-+and+Pixel-Level+Vision-Language+Alignment" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=DINOv2%20Meets%20Text%3A%20A%20Unified%20Framework%20for%20Image-%20and%20Pixel-Level%20Vision-Language%20Alignment&amp;body=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdino-txt-cvpr-2025%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdino-txt-cvpr-2025%2F&amp;title=DINOv2+Meets+Text%3A+A+Unified+Framework+for+Image-+and+Pixel-Level+Vision-Language+Alignment" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=DINOv2+Meets+Text%3A+A+Unified+Framework+for+Image-+and+Pixel-Level+Vision-Language+Alignment%20https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdino-txt-cvpr-2025%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fbaldassarrefe.github.io%2Fpublication%2Fdino-txt-cvpr-2025%2F&amp;title=DINOv2+Meets+Text%3A+A+Unified+Framework+for+Image-+and+Pixel-Level+Vision-Language+Alignment" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://baldassarrefe.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu248acd790418c168d502f2067d78835d_686148_270x270_fill_lanczos_center_3.png alt="Federico Baldassarre"></a><div class=media-body><h5 class=card-title><a href=https://baldassarrefe.github.io/>Federico Baldassarre</a></h5><h6 class=card-subtitle>Postdoctoral Researcher</h6><p class=card-text>Self-supervised learning, computer vision, and world models.</p><ul class=network-icon aria-hidden=true><li><a href=https://mailhide.io/e/nmVtbPdX target=_blank rel=noopener><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/baldassarreFe target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://twitter.com/BaldassarreFe target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://www.linkedin.com/in/federicobaldassarre target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href="https://scholar.google.com/citations?user=0iy5EucAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/publication/dinov3-2025/>DINOv3</a></li><li><a href=/publication/dino-world-2025/>Back to the Features: DINO as a Foundation for Video World Models</a></li><li><a href=/publication/capi-tmlr-2025/>Cluster and Predict Latents Patches for Improved Masked Image Modeling</a></li></ul></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">Â© 2025 Federico Baldassarre | License <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by></p></footer></div></div><script src=/js/vendor-bundle.min.b2240102cb8b24bcbe037562ce2ea60a.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.7f5ebaff62ae468cff8bb3dd1337bb9b.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js type=module></script></body></html>