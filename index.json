[{"authors":null,"categories":null,"content":"I’m a postdoctoral researcher at FAIR in Meta AI, Paris. My interests include self-supervised learning, computer vision, and video world models. During my PhD at KTH, Stockholm, I worked on the explainability of deep learning models, applied to computer vision and bioinformatics.\n","date":1755129600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1755129600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m a postdoctoral researcher at FAIR in Meta AI, Paris. My interests include self-supervised learning, computer vision, and video world models. During my PhD at KTH, Stockholm, I worked on the explainability of deep learning models, applied to computer vision and bioinformatics.","tags":null,"title":"Federico Baldassarre","type":"authors"},{"authors":["Oriane Siméoni","Huy V. Vo","Maximilian Seitzer","Federico Baldassarre","Maxime Oquab","Cijo Jose","Vasil Khalidov","Marc Szafraniec","Seungeun Yi","Michaël Ramamonjisoa","Francisco Massa","Daniel Haziza","Luca Wehrstedt","Jianyuan Wang","Timothée Darcet","Théo Moutakanni","Leonel Sentana","Claire Roberts","Andrea Vedaldi","Jamie Tolan","John Brandt","Camille Couprie","Julien Mairal","Hervé Jégou","Patrick Labatut","Piotr Bojanowski"],"categories":null,"content":"DINOv3 🦖🦖🦖 DINOv3 is a significant leap forward in self-supervised vision foundation models. This major release sets a new standard, offering stunning high-resolution dense features that are set to revolutionize various vision tasks.\nWhile we scaled both model size and training data, DINOv3’s true innovation lies in key architectural and training advancements.\nWhat’s inside DINOv3? DINOv3 is packed with features and techniques:\nA powerful 7B ViT foundation model, complemented by smaller distilled models. Trained on a massive dataset of 1.7 billion curated images, remarkably without any annotations, showcasing the power of self-supervision. The introduction of Gram anchoring, a novel technique that effectively addresses feature map degradation, a common challenge when training models of this scale for extended periods. Achieving high-resolution adaptation through the use of relative spatial coordinates and 2D RoPE, enabling exceptional performance on high-resolution inputs. Dense Feature Degradation and Gram Anchoring As we scaled up model size, number of images, and training duration, we observed two phenomena:\nDownstream classification accuracy continued to improve. Performance on dense tasks, which rely on rich, consistent feature maps, would rapidly drop. For example, see the comparison of ImageNet-1K classification accuracy vs. Pascal VOC segmentation mIoU:\nFrom qualitative observations, we notice that early during training, dense features are nicely localized and consistent within object boundaris. However, as training progresses, they become progressively more noisy.\nTo overcome this roadblock, we introduce Gram anchoring. Our goal is to achieve consistency found in early training iterations while retaining the semantic richness developed during late training. Therefore, we load an early checkpoint of the model being trained, and use is as a second teacher. Instead of forcing the features of the student to match the teacher features perfectly, which would be a too strong constraint, we distill the high-resolution pairwise patch similarity, i.e. the Gram matrix, from the teacher. Critically, this still allows the student features to evolve freely and improve, enabling unprecedented performance.\nUpon introducing Gram anchoring, we qualitatively observe an improvement in the feature maps:\nAnd quantitatively, we see a significant boost in performance on dense tasks:\nHigh-Resolution Adaptation Now that we can train the model for longer, we should also think about training it on high-resolution images. For the longest part, we feed the model images of size $256{\\times}256$, which we increase to $512{\\times}512$ and $768{\\times}768$ in the final training stages.\nOur constant-schedule training approach allows us to spin off a model specifically for high-resolution adaptation at any point. We seamlessly increase the input resolution while keeping Gram anchoring enabled. In our implementation, we use relative spatial coordinates in the range of $[0,1]^2$ to represent the position of each patch in the image for the 2D rotational positional encoding (RoPE). As a consequence, images of higher resolution are simply treated as an interpolation of the patch coordinates, without special handling.\nHow well does it work? Qualitative Visualization Compared to other vision foundation models, both self-supervised and not, DINOv3 features are more consistent and semantically rich. The quality of DINOv3’s feature maps is truly exceptional.\nIn the crowded market scene below, we visualize the cosine similarity between the patches marked with a red cross and all other patches.\nCompared to other vision foundation models, both self-supervised and not, DINOv3 patch features are more consistent, artifact-free, with no unwanted spilling around the edges of objects. Object contours look so precise, it appears like object segmentation is just one clustering step away!\nBelow, we visualize the first three PCA components of the features as RGB:\nAfter high-resolution adaptation, DINOv3 models generalize well to a wide range of resolutions, even higher than those seen during training. Forget Full-HD, it’s time for dense features in 4K!\nPerformance Evaluation Even with a frozen backbone, DINOv3 delivers massive performance gains across a variety of benchmarks. Some examples include:\nCOCO detection: achieving a state-of-the-art 66.1 mAP with a frozen backbone and detection model based on Plain-DETR. This is incredible! ADE20k segmentation: a linear 55.9 mIoU, a significant +6 improvement over previous self-supervised methods, and 63.0 mIoU with a decoder on top. 3D correspondence: impressive 64.4 recall on NAVI. Video tracking: a strong 83.3 J\u0026amp;F on DAVIS, thanks to its ability to handle high-resolution inputs. Below, is an example of segmentation tracking on a video. Object instances in the first frame of the video are segmented manually (thanks SAM 2!). Subsequent frames are automatically segmented by propagating labels from the first frame …","date":1755129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1755129600,"objectID":"36ad703bd43fcf091b2184c4cef0f773","permalink":"https://baldassarrefe.github.io/publication/dinov3-2025/","publishdate":"2025-08-14T00:00:00Z","relpermalink":"/publication/dinov3-2025/","section":"publication","summary":"**Preprint 2025** - Scaling vision SSL to 7B parameters and 1.7B images, achieving unprecedented patch feature quality.\n","tags":["Self-supervised Learning","Postdoc","DINO"],"title":"DINOv3","type":"publication"},{"authors":["Federico Baldassarre","Marc Szafraniec","Basile Terver","Vasil Khalidov","Francisco Massa","Yann LeCun","Patrick Labatut","Maximilian Seitzer","Piotr Bojanowski"],"categories":null,"content":"Given past frames, the model can predict future DINOv2 features autoregressively. How well does it do? It depends on the objects in motion and the context provided.\nIn the video below, the model can observe the taxi trajectory for long enough to predict that it will turn right.\nAutoregressive prediction of DINO-world with 10 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world. On the contrary, when given a too few frames, the model predicts that the taxi will continue straight. The blurry features towards the end are an artifact of the model’s uncertainty about the future due to the ambiguous context.\nAutoregressive prediction of DINO-world with only 4 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world. An early version of this work, without the action-conditioned fine-tuning and the planning tasks, was presented at the ICML 2025 Workshop on Building Physically Plausible World Models.\n","date":1752537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752537600,"objectID":"5b86686a6967e4a7301013061b23b7b4","permalink":"https://baldassarrefe.github.io/publication/dino-world-2025/","publishdate":"2025-07-15T00:00:00Z","relpermalink":"/publication/dino-world-2025/","section":"publication","summary":"**ICML Workshop 2025** - Learning physical world models in the latent space of DINOv2 from uncurated web videos.\n","tags":["Video world models","Postdoc","DINO"],"title":"Back to the Features: DINO as a Foundation for Video World Models","type":"publication"},{"authors":["Cijo Jose","Théo Moutakanni","Dahyun Kang","Federico Baldassarre","Timothée Darcet","Hu Xu","Daniel Li","Marc Szafraniec","Michaël Ramamonjisoa","Maxime Oquab","Oriane Siméoni","Huy V. Vo","Patrick Labatut","Piotr Bojanowski"],"categories":null,"content":" Twitter thread DINOv2 meets text at #CVPR 2025! Why choose between high-quality DINO features and CLIP-style vision-language alignment? Pick both with dino.txt 🦖📖\nWe align frozen DINOv2 features with text captions, obtaining both image-level and patch-level alignment at a minimal cost. [1/N] pic.twitter.com/7BTwLxqXNG\n— Federico Baldassarre (@BaldassarreFe) June 14, 2025 Video presentation ","date":17496e5,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":17496e5,"objectID":"496ddea7a9b6018400c314b417bb5bfb","permalink":"https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/","publishdate":"2025-05-25T00:00:00Z","relpermalink":"/publication/dino-txt-cvpr-2025/","section":"publication","summary":"**CVPR 2025** - Locked-image tuning for vision-language alignment using a DINOv2 backbone and a few tricks on top.\n","tags":["Vision-language alignment","LLMs","CLIP","Postdoc","DINO"],"title":"DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment","type":"publication"},{"authors":["Timothée Darcet","Federico Baldassarre","Maxime Oquab","Julien Mairal","Piotr Bojanowski"],"categories":null,"content":"Here is a tweet-sized summary by the lead author:\nWant strong SSL, but not the complexity of DINOv2?\nCAPI: Cluster and Predict Latents Patches for Improved Masked Image Modeling. pic.twitter.com/gOB4QO7DKn\n— TimDarcet (@TimDarcet) February 14, 2025 ","date":1748736e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748736e3,"objectID":"069fa172108f1ad425a430965778a00d","permalink":"https://baldassarrefe.github.io/publication/capi-tmlr-2025/","publishdate":"2025-05-25T00:00:00Z","relpermalink":"/publication/capi-tmlr-2025/","section":"publication","summary":"**TMLR 2025** - Stable training of dense image representations using a clustering loss on ViT patch tokens.\n","tags":["Self-supervised Learning","Clustering","Postdoc"],"title":"Cluster and Predict Latents Patches for Improved Masked Image Modeling","type":"publication"},{"authors":["Federico Baldassarre","Alaaeldin El-Nouby","Hervé Jégou"],"categories":null,"content":"Video presentation ","date":1685836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685836800,"objectID":"319369d58a1ce5d22fd01aa47114f6c1","permalink":"https://baldassarrefe.github.io/publication/semantic-compression-icassp-2023/","publishdate":"2020-08-23T00:00:00Z","relpermalink":"/publication/semantic-compression-icassp-2023/","section":"publication","summary":"**ICASSP 2023** - Control the bit rate allocation of a a PQ-VAE using semantic information from DINO, in collaboration with Meta AI in Paris.\n","tags":["Image Compression","Self-supervised Learning","Transformers","Internship"],"title":"Variable Rate Allocation for Vector-Quantized Autoencoders","type":"publication"},{"authors":["Hao Hu","Federico Baldassarre","Hossein Azizpour"],"categories":null,"content":"","date":1668988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668988800,"objectID":"488512790eaffac063fa007c254fdf03","permalink":"https://baldassarrefe.github.io/publication/learnable-masked-token-ecmlpkdd-2022/","publishdate":"2020-08-23T00:00:00Z","relpermalink":"/publication/learnable-masked-token-ecmlpkdd-2022/","section":"publication","summary":"**ECML PKDD 2022** - Learnable masked tokens reduce overfitting when training transformers in low-data regime.\n","tags":["Transformers"],"title":"Learnable Masked Tokens for Improved Transferability of Self-Supervised Vision Transformers","type":"publication"},{"authors":["Federico Baldassarre","Quentin Debard","Gonzalo Fiz Pontiveros","Tri Kurniawan Wijaya"],"categories":null,"content":"Video presentation ","date":1668988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668988800,"objectID":"8a344d2056adcd2084a4446cd3d529ee","permalink":"https://baldassarrefe.github.io/publication/deepfake-explanations-bmvc-2022/","publishdate":"2020-08-23T00:00:00Z","relpermalink":"/publication/deepfake-explanations-bmvc-2022/","section":"publication","summary":"**BMVC 2022** - Quantitative metrics for evaluating explanations of video DeepFake detectors, in collaboration with the Huawei Ireland Research Center.\n","tags":["DeepFakes","Explainability","Video Deep Learning","Internship"],"title":"Quantitative Metrics for Evaluating Explanations of Video DeepFake Detectors","type":"publication"},{"authors":["Federico Baldassarre","Hossein Azizpour"],"categories":null,"content":"","date":1651190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651190400,"objectID":"ac2e2f648406772d5e927740ca9f5a41","permalink":"https://baldassarrefe.github.io/publication/object-centric-ssl-iclrw-2022/","publishdate":"2020-08-23T00:00:00Z","relpermalink":"/publication/object-centric-ssl-iclrw-2022/","section":"publication","summary":"**ICLR Workshop 2022** - How to learn object-centric representations using contrastive learning on multi-object datasets?\n","tags":["Object-centric representation","Self-supervised learning"],"title":"Towards Self-Supervised Learning of Global and Object-Centric Representations","type":"publication"},{"authors":["Andriy Kryshtafovych","John Moult","Wendy Billings","Dennis Della Corte","Krzysztof Fidelis","Sohee Kwon","Kliment Olechnovič","Chaok Seok","Česlovas Venclovas","Jonghun Won","CASP-COVID participants"],"categories":null,"content":"","date":1630281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630281600,"objectID":"451677c5746c8682771f3d70d3974c90","permalink":"https://baldassarrefe.github.io/publication/casp-covid19-proteins-2021/","publishdate":"2021-10-10T00:00:00Z","relpermalink":"/publication/casp-covid19-proteins-2021/","section":"publication","summary":"**Proteins: Structure, Function, and Bioinformatics** - CASP community-wide experiment on modeling SARS-CoV-2 proteins causing the COVID-19 pandemic.\n","tags":["Bioinformatics","Protein Quality Assessment","CASP","SARS-Cov-2","COVID-19"],"title":"Modeling SARS-CoV-2 proteins in the CASP-commons experiment","type":"publication"},{"authors":["Federico Baldassarre","Kevin Smith","Josephine Sullivan","Hossein Azizpour"],"categories":null,"content":"Video presentation ","date":1598140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598140800,"objectID":"78c13968a60b8bb01b584ade4c361e52","permalink":"https://baldassarrefe.github.io/publication/weakly-sup-relationship-detection-eccv-2020/","publishdate":"2020-08-23T00:00:00Z","relpermalink":"/publication/weakly-sup-relationship-detection-eccv-2020/","section":"publication","summary":"**ECCV 2020** - A novel weakly-supervised method for visual relationship detection that relies on minimal image-level predicate labels, graph networks, and attribution-based explanations.\n","tags":["Graph Neural Networks","Explainability","Visual Relationship Detection","Weakly-supervised Learning"],"title":"Explanation-based Weakly-supervised Learning of Visual Relations with Graph Networks","type":"publication"},{"authors":["Federico Baldassarre","David Menéndez Hurtado","Arne Elofsson","Hossein Azizpour"],"categories":null,"content":"","date":1597104e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597104e3,"objectID":"97c0a9c200a9b83582f73773f74b189f","permalink":"https://baldassarrefe.github.io/publication/graphqa-bioinformatics-2020/","publishdate":"2020-08-11T00:00:00Z","relpermalink":"/publication/graphqa-bioinformatics-2020/","section":"publication","summary":"**Bioinformatics 2020** - A novel method for protein quality assessment based on a graphical representation of tertiary structures and graph convolutional networks.\n","tags":["Graph Neural Networks","Protein Quality Assessment","CASP","Bioinformatics"],"title":"GraphQA: Protein Model Quality Assessment Using Graph Convolutional Networks","type":"publication"},{"authors":["Federico Baldassarre","Hossein Azizpour"],"categories":null,"content":"","date":1559260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559260800,"objectID":"6a3d41b3c49df84bbbccddcec5c7f561","permalink":"https://baldassarrefe.github.io/publication/explainability-gcn-icmlw-2019/","publishdate":"2019-05-31T00:00:00Z","relpermalink":"/publication/explainability-gcn-icmlw-2019/","section":"publication","summary":"**ICML Workshop 2019** - How do Sensitivity Analysis, Guided Backpropagation, and Layer-Wise Relevance Propagation behave when applied to Graph Neural Networks?\n","tags":["Graph Neural Networks","Explainability"],"title":"Explainability Techniques for Graph Convolutional Networks","type":"publication"},{"authors":["Federico Baldassarre"],"categories":null,"content":"","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530144e3,"objectID":"a027f22043ede25740cb0c4e684054f2","permalink":"https://baldassarrefe.github.io/publication/zalando-master-thesis-2018/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/publication/zalando-master-thesis-2018/","section":"publication","summary":"**Master's Thesis** - Given the picture of a person and of a clothing item, generate the image of the person wearing it. Work performed during an internship at Zalando Research, Berlin.","tags":["Image generation","Fashion","Thesis"],"title":"Morphing Architectures for Pose-based Image Generation of People in Clothing","type":"publication"},{"authors":["Federico Baldassarre","Diego Gonzalez Morín","Lucas Rodés-Guirao"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"efa09da33c28db0277816ef76b813e84","permalink":"https://baldassarrefe.github.io/publication/deep-koalarization-2017/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/deep-koalarization-2017/","section":"publication","summary":"**Technical Report** - Grayscale image colorization using a CNN conditione on Inception-ResNet-v2. Deep Learning course at KTH, best project award.\n","tags":["MSc project","Image colorization"],"title":"Deep Koalarization","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4e341564189a2e1a1cb21603ba4c1d9e","permalink":"https://baldassarrefe.github.io/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"","tags":null,"title":"Experience","type":"landing"},{"authors":null,"categories":null,"content":" 2025 October 2025 Guest lecture on DINOv3 to deep learning students at Chalmers University, Gothenburg, Sweden 🇸🇪\nSeptember 2025 Honored to be invited to present DINOv3 at the Foundation Model workshop at the Geneva Institute of Theoretical Science 🇨🇭\nSeptember 2025 Presenting my postdoc contributions (CAPI, dino.txt, DINO-world and DINOv3) to students of the WASP AI program at the University of Linköping, Sweden 🇸🇪\nAugust 2025 Announcing DINOv3, a major release that raises the bar of self-supervised vision foundation models 🦖🦖🦖\nJuly 2025 Publishing DINO-world, a latent video world model trained in the feature space of DINOv2. Also presented at ICML 2025 in Vancouver 🍁\nJune 2025 See you in Nashville, Tennessee, for CVPR 2025 🤠🪕\nMay 2025 Our work on scalable pre-training of dense image representations, CAPI, is accepted at TMLR. Congrats to Timothée for leading this work!\nFeb 2025 Our “DINO.txt” paper on aligning vision and language representations is accepted at CVPR 2025.\n2024 Oct 2024 Presenting my work on obtaining object-centric representations by clustering DINOv2 features at the FAIR conference in New York. 2023 Sept 2023 I’m moving to Paris to join the DINO team at Meta AI as a postdoctoral researcher 🦖\nJun 2023 PhD Graduation at KTH, Stockholm 🎓🎉\nFeb 2023 Our work on semantic image compression with quantized autoencoders is accepted at ICASSP 2023 (Meta AI internship).\n2022 Sept 2022 Our work on quantitative metrics for evaluating DeepFake explanations is accepted at BMVC 2022 (Huawei Research internship).\nAug 2022 My master student Erik Dao graduated with a thesis on sparsity in transformers for object detection (DETR).\nJun 2022 I’m starting a research internship at FAIR (Meta AI) in Paris under the supervision of Hervé Jégou.\nMar 2022 Our work on object-centric learning is accepted at the ICLR 2022 Workshop on the Elements of Reasoning: Objects, Structure, and Causality.\nJan 2022 Gave a presentation on evaluating DeepFake explanations at Digital Future’s Machine Learning Day.\n2021 Sept 2021 Started a summer internship at Huawei Research in Dublin!\nJan 2021 My master student Jindong Wu graduated with a thesis on pooling strategies for Graph Networks.\n2020 and older Aug 2020 Proud to be nominated as an outstanding reviewer at ECCV 2020.\nJun 2020 I hosted a full-day workshop for my department on how get the most from our in-house GPU cluster.\nMay 2019 Our work on GNN Explainability is accepted at the ICML 2019 Workshop on Learning and Reasoning with Graph-Structured Data.\nJan 2019 Presented my Master’s thesis on image generation for clothes virtual try-on at the Northern Lights Deep Learning Workshop.\nSept 2018 Starting my PhD journey at KTH under the supervision of Prof. Hossein Azizpour.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"https://baldassarrefe.github.io/news/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"2025 October 2025 Guest lecture on DINOv3 to deep learning students at Chalmers University, Gothenburg, Sweden 🇸🇪\nSeptember 2025 Honored to be invited to present DINOv3 at the Foundation Model workshop at the Geneva Institute of Theoretical Science 🇨🇭","tags":null,"title":"News","type":"page"}]