<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Paper-Conference | Federico Baldassarre</title><link>https://baldassarrefe.github.io/publication-type/paper-conference/</link><atom:link href="https://baldassarrefe.github.io/publication-type/paper-conference/index.xml" rel="self" type="application/rss+xml"/><description>Paper-Conference</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 15 Jul 2025 00:00:00 +0000</lastBuildDate><image><url>https://baldassarrefe.github.io/media/icon_hu2699d9b9ce559aed68fb11b26d693084_18909_512x512_fill_lanczos_center_3.png</url><title>Paper-Conference</title><link>https://baldassarrefe.github.io/publication-type/paper-conference/</link></image><item><title>Back to the Features: DINO as a Foundation for Video World Models</title><link>https://baldassarrefe.github.io/publication/dino-world-2025/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/dino-world-2025/</guid><description>&lt;p>Given past frames, the model can predict future DINOv2 features autoregressively.
How well does it do? It depends on the objects in motion and the context provided.&lt;/p>
&lt;p>In the video below, the model can observe the taxi trajectory for long enough to predict that it will turn right.&lt;/p>
&lt;figure id="figure-autoregressive-prediction-of-dino-world-with-10-frames-of-context-top-row-ground-truth-video-middle-row-per-frame-dinov2-features-bottom-row-autoregressive-predictions-of-dino-world">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Autoregressive prediction of DINO-world with 10 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world." srcset="
/publication/dino-world-2025/autoregressive_taxi_10_hue4871d0d37b3e474f89ea0efdc740fbe_2238195_427fefce7f2c57f1a98b4845df2780f7.webp 400w,
/publication/dino-world-2025/autoregressive_taxi_10_hue4871d0d37b3e474f89ea0efdc740fbe_2238195_94ce584be6b1d409b3080522cb194a44.webp 760w,
/publication/dino-world-2025/autoregressive_taxi_10_hue4871d0d37b3e474f89ea0efdc740fbe_2238195_1200x1200_fit_q80_h2_lanczos_3.webp 1200w"
src="https://baldassarrefe.github.io/publication/dino-world-2025/autoregressive_taxi_10_hue4871d0d37b3e474f89ea0efdc740fbe_2238195_427fefce7f2c57f1a98b4845df2780f7.webp"
width="760"
height="148"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Autoregressive prediction of DINO-world with 10 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world.
&lt;/figcaption>&lt;/figure>
&lt;p>On the contrary, when given a too few frames, the model predicts that the taxi will continue straight.
The blurry features towards the end are an artifact of the model&amp;rsquo;s uncertainty about the future due to the ambiguous context.&lt;/p>
&lt;figure id="figure-autoregressive-prediction-of-dino-world-with-only-4-frames-of-context-top-row-ground-truth-video-middle-row-per-frame-dinov2-features-bottom-row-autoregressive-predictions-of-dino-world">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Autoregressive prediction of DINO-world with only 4 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world." srcset="
/publication/dino-world-2025/autoregressive_taxi_4_hu70925a527a10997dd20e96e54ca285dc_2298178_fd0dcab6282978c42d276745cdf39c9b.webp 400w,
/publication/dino-world-2025/autoregressive_taxi_4_hu70925a527a10997dd20e96e54ca285dc_2298178_d4d370e25d1a68dfe956ad7ba98555e2.webp 760w,
/publication/dino-world-2025/autoregressive_taxi_4_hu70925a527a10997dd20e96e54ca285dc_2298178_1200x1200_fit_q80_h2_lanczos_3.webp 1200w"
src="https://baldassarrefe.github.io/publication/dino-world-2025/autoregressive_taxi_4_hu70925a527a10997dd20e96e54ca285dc_2298178_fd0dcab6282978c42d276745cdf39c9b.webp"
width="760"
height="149"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Autoregressive prediction of DINO-world with only 4 frames of context. Top row: ground-truth video. Middle row: per-frame DINOv2 features. Bottom row: autoregressive predictions of DINO-world.
&lt;/figcaption>&lt;/figure>
&lt;hr>
&lt;p>&lt;span style="font-size: .8rem">An early version of this work, without the action-conditioned fine-tuning and the planning tasks,
was presented at the ICML 2025 Workshop on
&lt;a href="https://physical-world-modeling.github.io/" target="_blank" rel="noopener">Building Physically Plausible World Models&lt;/a>.&lt;/span>&lt;/p></description></item><item><title>DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment</title><link>https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/</link><pubDate>Wed, 11 Jun 2025 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/</guid><description>&lt;!-- Example of zero-shot segmentation using DINO.txt: -->
&lt;!--
&lt;figure id="figure-zero-shot-segmentation-using-dinotxt">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Zero-shot segmentation using DINO.txt" srcset="
/publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_b118d9c55be2d8f0ea4e22e4c448a407.webp 400w,
/publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_e46ca33f9f7ec699094abe9dfbad06e5.webp 760w,
/publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_1200x1200_fit_q80_h2_lanczos_3.webp 1200w"
src="https://baldassarrefe.github.io/publication/dino-txt-cvpr-2025/zero-shot-segmentation_hu5f7f2fb3d4eefd0937ffa50dafd3c6da_1874313_b118d9c55be2d8f0ea4e22e4c448a407.webp"
width="760"
height="254"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Zero-shot segmentation using DINO.txt
&lt;/figcaption>&lt;/figure>
-->
&lt;h2 id="twitter-thread">Twitter thread&lt;/h2>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">DINOv2 meets text at &lt;a href="https://twitter.com/hashtag/CVPR?src=hash&amp;amp;ref_src=twsrc%5Etfw">#CVPR&lt;/a> 2025! Why choose between high-quality DINO features and CLIP-style vision-language alignment? Pick both with dino.txt ðŸ¦–ðŸ“–&lt;br>&lt;br>We align frozen DINOv2 features with text captions, obtaining both image-level and patch-level alignment at a minimal cost. [1/N] &lt;a href="https://t.co/7BTwLxqXNG">pic.twitter.com/7BTwLxqXNG&lt;/a>&lt;/p>&amp;mdash; Federico Baldassarre (@BaldassarreFe) &lt;a href="https://twitter.com/BaldassarreFe/status/1933975376377892974?ref_src=twsrc%5Etfw">June 14, 2025&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;h2 id="video-presentation">Video presentation&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/qNsAgsvhbw4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Variable Rate Allocation for Vector-Quantized Autoencoders</title><link>https://baldassarrefe.github.io/publication/semantic-compression-icassp-2023/</link><pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/semantic-compression-icassp-2023/</guid><description>&lt;h2 id="video-presentation">Video presentation&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/JggN9r1FFMw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Learnable Masked Tokens for Improved Transferability of Self-Supervised Vision Transformers</title><link>https://baldassarrefe.github.io/publication/learnable-masked-token-ecmlpkdd-2022/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/learnable-masked-token-ecmlpkdd-2022/</guid><description/></item><item><title>Quantitative Metrics for Evaluating Explanations of Video DeepFake Detectors</title><link>https://baldassarrefe.github.io/publication/deepfake-explanations-bmvc-2022/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/deepfake-explanations-bmvc-2022/</guid><description>&lt;h2 id="video-presentation">Video presentation&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/KAwigBozHeM" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Towards Self-Supervised Learning of Global and Object-Centric Representations</title><link>https://baldassarrefe.github.io/publication/object-centric-ssl-iclrw-2022/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/object-centric-ssl-iclrw-2022/</guid><description/></item><item><title>Explanation-based Weakly-supervised Learning of Visual Relations with Graph Networks</title><link>https://baldassarrefe.github.io/publication/weakly-sup-relationship-detection-eccv-2020/</link><pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/weakly-sup-relationship-detection-eccv-2020/</guid><description>&lt;h2 id="video-presentation">Video presentation&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/7n7ffyRTuog" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></description></item><item><title>Explainability Techniques for Graph Convolutional Networks</title><link>https://baldassarrefe.github.io/publication/explainability-gcn-icmlw-2019/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://baldassarrefe.github.io/publication/explainability-gcn-icmlw-2019/</guid><description/></item></channel></rss>